{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040caa15",
   "metadata": {},
   "source": [
    "# feature engineering (all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f298769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Total unique NER characters: 1692\n",
      "‚úÖ Covered by normalized list: 1692\n",
      "‚ùå Missing from normalized list: 0\n",
      "Missing examples: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load file\n",
    "ner_df = pd.read_csv(\"ner_sentence_characters.csv\")\n",
    "norm_df = pd.read_csv(\"normalized_characters.csv\")\n",
    "\n",
    "# Ubah string list jadi list beneran\n",
    "ner_df['characters'] = ner_df['characters'].apply(literal_eval)\n",
    "norm_df['characters'] = norm_df['characters'].apply(literal_eval)\n",
    "\n",
    "# Ambil semua karakter unik dari kedua file\n",
    "all_ner_chars = set([char.lower() for sublist in ner_df['characters'] for char in sublist])\n",
    "all_norm_chars = set([char.lower() for sublist in norm_df['characters'] for char in sublist])\n",
    "\n",
    "# Cari karakter yang ada di ner tapi belum masuk ke file normalized\n",
    "missing = all_ner_chars - all_norm_chars\n",
    "\n",
    "# Print hasilnya\n",
    "print(f\"üîç Total unique NER characters: {len(all_ner_chars)}\")\n",
    "print(f\"‚úÖ Covered by normalized list: {len(all_norm_chars)}\")\n",
    "print(f\"‚ùå Missing from normalized list: {len(missing)}\")\n",
    "print(\"Missing examples:\", list(missing)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da57bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Total unique normalized characters: 1617\n",
      "‚úÖ Covered by alias list: 1615\n",
      "‚ùå Missing from alias list: 2\n",
      "Missing examples: ['putroe', 'telangkai']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load files\n",
    "norm_df = pd.read_csv(\"normalized_characters.csv\")\n",
    "alias_df = pd.read_csv(\"alias_clusters.csv\")\n",
    "\n",
    "# Parse kolom jadi list beneran\n",
    "norm_df['normalized_characters'] = norm_df['normalized_characters'].apply(literal_eval)\n",
    "alias_df['aliases'] = alias_df['aliases'].apply(literal_eval)\n",
    "\n",
    "# Ambil semua karakter unik dari normalized\n",
    "all_norm_chars = set([char.lower() for sublist in norm_df['normalized_characters'] for char in sublist])\n",
    "\n",
    "# Ambil semua alias yang sudah dicatat\n",
    "all_aliases = set([alias.lower() for sublist in alias_df['aliases'] for alias in sublist])\n",
    "\n",
    "# Cari yang belum masuk ke alias cluster\n",
    "missing = all_norm_chars - all_aliases\n",
    "\n",
    "# Print hasilnya\n",
    "print(f\"üîç Total unique normalized characters: {len(all_norm_chars)}\")\n",
    "print(f\"‚úÖ Covered by alias list: {len(all_aliases & all_norm_chars)}\")\n",
    "print(f\"‚ùå Missing from alias list: {len(missing)}\")\n",
    "print(\"Missing examples:\", list(missing)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbc912",
   "metadata": {},
   "source": [
    "## merge alias clusters with NER normalized characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09048e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved as 'normalized_characters_expanded.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# === 1. Load the file ===\n",
    "df = pd.read_csv(\"normalized_characters.csv\")\n",
    "\n",
    "# convert the stringified lists to actual Python lists\n",
    "df[\"characters\"]            = df[\"characters\"].apply(ast.literal_eval)\n",
    "df[\"normalized_characters\"] = df[\"normalized_characters\"].apply(ast.literal_eval)\n",
    "\n",
    "# === 2. Explode so one mention per row ===\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    sid   = row[\"story_id\"]\n",
    "    sent  = row[\"sentence_id\"]\n",
    "    chars = row[\"characters\"]\n",
    "    norm  = row[\"normalized_characters\"]\n",
    "\n",
    "    # zip keeps the original ‚Üî normalized pairing\n",
    "    for orig, normed in zip(chars, norm):\n",
    "        rows.append({\n",
    "            \"story_id\": sid,\n",
    "            \"sentence_id\": sent,\n",
    "            \"characters\": [orig],               # keep list format with single item\n",
    "            \"normalized_characters\": [normed]\n",
    "        })\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "\n",
    "# === 3. Save the expanded file ===\n",
    "out.to_csv(\"normalized_characters_expanded.csv\", index=False)\n",
    "print(\"‚úÖ Saved as 'normalized_characters_expanded.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2fb330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved as 'alias_sentence_map.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# === 1. Load data ===\n",
    "exp = pd.read_csv(\"normalized_characters_expanded.csv\")   # already 1-token per row\n",
    "ali = pd.read_csv(\"alias_clusters.csv\")\n",
    "\n",
    "# parse list-columns\n",
    "exp[\"normalized_characters\"] = exp[\"normalized_characters\"].apply(ast.literal_eval)\n",
    "ali[\"aliases\"]               = ali[\"aliases\"].apply(ast.literal_eval)\n",
    "\n",
    "# unwrap the single-item list into a plain string (lower-case for safe matching)\n",
    "exp[\"norm_tok\"] = exp[\"normalized_characters\"].str[0].str.lower()\n",
    "\n",
    "# === 2. Build the sentence-ID list for each alias cluster ===\n",
    "rows = []\n",
    "for _, a in ali.iterrows():\n",
    "    sid     = a[\"story_id\"]\n",
    "    person  = a[\"person\"]\n",
    "    aliases = [t.lower() for t in a[\"aliases\"]]          # match on normalized tokens\n",
    "\n",
    "    sent_ids = (\n",
    "        exp[(exp[\"story_id\"] == sid) & (exp[\"norm_tok\"].isin(aliases))]\n",
    "        [\"sentence_id\"]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    sent_ids.sort()\n",
    "\n",
    "    # join into comma-separated string (e.g., \"3,5,7\")\n",
    "    rows.append({\n",
    "        \"story_id\": sid,\n",
    "        \"person\": person,\n",
    "        \"aliases\": a[\"aliases\"],          # keep list form\n",
    "        \"sentence_id\": \", \".join(map(str, sent_ids)) if sent_ids else \"\"\n",
    "    })\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "\n",
    "# === 3. Save result ===\n",
    "out.to_csv(\"alias_sentence_map.csv\", index=False)\n",
    "print(\"‚úÖ Saved as 'alias_sentence_map.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1c20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved as 'alias_sentence_original.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# === 1. Load data ===\n",
    "exp  = pd.read_csv(\"normalized_characters_expanded.csv\")\n",
    "ali  = pd.read_csv(\"alias_clusters.csv\")\n",
    "\n",
    "# Parse list-columns\n",
    "exp[\"normalized_characters\"] = exp[\"normalized_characters\"].apply(ast.literal_eval)\n",
    "exp[\"characters\"]            = exp[\"characters\"].apply(ast.literal_eval)\n",
    "ali[\"aliases\"]               = ali[\"aliases\"].apply(ast.literal_eval)\n",
    "\n",
    "# Extract single tokens for quick matching\n",
    "exp[\"norm_tok\"]      = exp[\"normalized_characters\"].str[0].str.lower()\n",
    "exp[\"orig_mention\"]  = exp[\"characters\"].str[0]             # keep original form\n",
    "\n",
    "# === 2. Build the mapping ===\n",
    "rows = []\n",
    "for _, a in ali.iterrows():\n",
    "    sid      = a[\"story_id\"]\n",
    "    person   = a[\"person\"]\n",
    "    aliases  = [t.lower() for t in a[\"aliases\"]]            # case-insensitive\n",
    "\n",
    "    sub = exp[\n",
    "        (exp[\"story_id\"] == sid) &\n",
    "        (exp[\"norm_tok\"].isin(aliases))\n",
    "    ]\n",
    "\n",
    "    sent_ids      = sorted(sub[\"sentence_id\"].unique().tolist())\n",
    "    orig_mentions = (\n",
    "        sub[[\"sentence_id\", \"orig_mention\"]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(\"sentence_id\")[\"orig_mention\"]\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"story_id\"        : sid,\n",
    "        \"person\"          : person,\n",
    "        \"aliases\"         : a[\"aliases\"],\n",
    "        \"sentence_id\"     : \", \".join(map(str, sent_ids)) if sent_ids else \"\",\n",
    "        \"original_mention\": \", \".join(orig_mentions)        if orig_mentions else \"\"\n",
    "    })\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "\n",
    "# === 3. Save result ===\n",
    "out.to_csv(\"alias_sentence_original.csv\", index=False)\n",
    "print(\"‚úÖ Saved as 'alias_sentence_original.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9049aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved as 'alias_with_augmented_aliases.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# === 1. Load the file made earlier ===\n",
    "df = pd.read_csv(\"alias_sentence_original.csv\")\n",
    "\n",
    "# Parse the list-in-string from the aliases column ‚Üí real Python list\n",
    "df[\"aliases\"] = df[\"aliases\"].apply(lambda s: ast.literal_eval(s) if isinstance(s, str) else [])\n",
    "\n",
    "# Helper to preserve order while deduplicating\n",
    "def uniq(seq):\n",
    "    seen = set()\n",
    "    out  = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "# === 2. Merge the original mentions into aliases (normalised to lowercase) ===\n",
    "def add_mentions(row):\n",
    "    aliases = [a.lower() for a in row[\"aliases\"]]              # baseline list\n",
    "    \n",
    "    # split \"Tuhanku, Tuhan, Tuhan\" ‚Üí [\"Tuhanku\", \"Tuhan\", \"Tuhan\"]\n",
    "    mentions = [m.strip() for m in str(row[\"original_mention\"]).split(\",\") if m.strip()]\n",
    "    mentions = [m.lower() for m in mentions]                   # normalise\n",
    "    \n",
    "    merged = uniq(aliases + mentions)                          # keep order / dedup\n",
    "    return merged\n",
    "\n",
    "df[\"aliases\"] = df.apply(add_mentions, axis=1)\n",
    "\n",
    "# === 3. Drop the original_mention column if you no longer need it ===\n",
    "df = df.drop(columns=[\"original_mention\"])\n",
    "\n",
    "# === 4. Save the result ===\n",
    "df.to_csv(\"alias_with_augmented_aliases.csv\", index=False)\n",
    "print(\"‚úÖ Saved as 'alias_with_augmented_aliases.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30770c",
   "metadata": {},
   "source": [
    "## merge alias + sentences with the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5aeb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved as 'alias_sentence_text.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# ‚îÄ‚îÄ 1. LOAD CLEANED-UP ALIAS TABLE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "alias_df = pd.read_csv(\"alias_with_augmented_aliases.csv\")   # story_id | person | aliases | sentence_id\n",
    "alias_df[\"aliases\"] = alias_df[\"aliases\"].apply(ast.literal_eval)\n",
    "\n",
    "def split_ids(val):\n",
    "    if pd.isna(val) or str(val).strip() == \"\":\n",
    "        return []\n",
    "    return [int(x) for x in str(val).split(\",\") if str(x).strip().isdigit()]\n",
    "\n",
    "alias_df[\"sentence_ids\"] = alias_df[\"sentence_id\"].apply(split_ids)\n",
    "\n",
    "# ‚îÄ‚îÄ 2. LOAD WORD-LEVEL TOKEN TABLE & BUILD FULL SENTENCES ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "tok_df = pd.read_csv(\"tokenized_sentences.csv\")              # story_id | judul | sentence_id | word\n",
    "\n",
    "# keep only rows where word is a real token\n",
    "tok_df = tok_df.dropna(subset=[\"word\"]).copy()\n",
    "tok_df[\"word\"] = tok_df[\"word\"].astype(str)                 # ensure str type\n",
    "\n",
    "sent_df = (\n",
    "    tok_df.groupby([\"story_id\", \"sentence_id\"])[\"word\"]\n",
    "          .agg(\" \".join)                                     # join tokens with space\n",
    "          .reset_index(name=\"text\")\n",
    ")\n",
    "\n",
    "sent_lookup = {(r.story_id, r.sentence_id): r.text for r in sent_df.itertuples()}\n",
    "\n",
    "# ‚îÄ‚îÄ 3. EXPAND alias_df (one row per sentence) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "rows = []\n",
    "for r in alias_df.itertuples():\n",
    "    for sid in r.sentence_ids:\n",
    "        rows.append({\n",
    "            \"story_id\"   : r.story_id,\n",
    "            \"person\"     : r.person,\n",
    "            \"aliases\"    : r.aliases,\n",
    "            \"sentence_id\": sid,\n",
    "            \"text\"       : sent_lookup.get((r.story_id, sid), \"\")\n",
    "        })\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "\n",
    "# ‚îÄ‚îÄ 4. SAVE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "out.to_csv(\"alias_sentence_text.csv\", index=False)\n",
    "print(\"‚úÖ Saved as 'alias_sentence_text.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b996b54",
   "metadata": {},
   "source": [
    "## add mention count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f937b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  mention_count kini dihitung per tokoh & disimpan di 'alias_sentence_features_revised.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ‚ïê‚ïê‚ïê 1. LOAD DATA ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "alias_df = pd.read_csv(\"alias_sentence_text.csv\")   # story_id | person | aliases | sentence_id | text\n",
    "token_df = pd.read_csv(\"tokenized_sentences.csv\")   # story_id | judul  | sentence_id | word\n",
    "\n",
    "alias_df[\"aliases\"] = alias_df[\"aliases\"].apply(ast.literal_eval)\n",
    "token_df  = token_df.dropna(subset=[\"word\"]).copy()\n",
    "token_df[\"word\"] = token_df[\"word\"].astype(str)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê 2. PREP TOKENS PER CERITA ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "story_tokens = defaultdict(list)        # {story_id: [token1, token2, ...]}\n",
    "for r in token_df.itertuples():\n",
    "    story_tokens[r.story_id].append(r.word.lower())\n",
    "\n",
    "story_strings = {sid: \" \".join(tok) for sid, tok in story_tokens.items()}\n",
    "\n",
    "# ‚ïê‚ïê‚ïê 3. HITUNG mention_count PER TOKOH (story-level) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "mention_dict = {}   # key = (story_id, person) -> total count\n",
    "\n",
    "# groupby agar satu tokoh (story_id, person) diproses sekali\n",
    "for (sid, person), sub in alias_df.groupby([\"story_id\", \"person\"]):\n",
    "    # satukan semua alias dari baris-baris tokoh tsb\n",
    "    alias_set = set()\n",
    "    for alist in sub[\"aliases\"]:\n",
    "        alias_set.update([a.lower() for a in alist])\n",
    "\n",
    "    toks = story_tokens[sid]\n",
    "    txt  = story_strings[sid]\n",
    "\n",
    "    count = 0\n",
    "    for a in alias_set:\n",
    "        if len(a.split()) == 1:                 # single-word alias\n",
    "            count += toks.count(a)\n",
    "        else:                                   # multi-word alias\n",
    "            count += len(re.findall(r\"\\b\" + re.escape(a) + r\"\\b\", txt))\n",
    "\n",
    "    mention_dict[(sid, person)] = count\n",
    "\n",
    "# ‚ïê‚ïê‚ïê 4. TEMPEL KE TABEL alias_sentence_text ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "alias_df[\"mention_count\"] = alias_df.apply(\n",
    "    lambda r: mention_dict[(r.story_id, r.person)], axis=1\n",
    ")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê 5. (OPSI) URUTKAN & SIMPAN ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "alias_df = alias_df.sort_values([\"story_id\", \"sentence_id\", \"person\"])\n",
    "alias_df.to_csv(\"alias_sentence_features_revised.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ  mention_count kini dihitung per tokoh & disimpan di 'alias_sentence_features_revised.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccabce",
   "metadata": {},
   "source": [
    "## add word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9f55cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Kolom word_count (total kata per tokoh di cerita) sudah ditambahkan dan disimpan ke 'alias_sentence_features_wc.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# ‚îÄ‚îÄ 1. LOAD file hasil sebelumnya ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = pd.read_csv(\"alias_sentence_features_revised.csv\")   # story_id | person | aliases | sentence_id | text | mention_count\n",
    "df[\"aliases\"] = df[\"aliases\"].apply(ast.literal_eval)\n",
    "\n",
    "# ‚îÄ‚îÄ 2. HITUNG panjang setiap kalimat (jumlah kata) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df[\"sent_len\"] = df[\"text\"].str.split().str.len()         # panjang per-kalimat\n",
    "\n",
    "# ‚îÄ‚îÄ 3. AGREGASI word_count per TOKOH (story-level) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "total_wc = (                                              # {(story_id, person): total_words}\n",
    "    df.groupby([\"story_id\", \"person\"])[\"sent_len\"]\n",
    "      .sum()\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "df[\"word_count\"] = df.apply(\n",
    "    lambda r: total_wc[(r.story_id, r.person)], axis=1\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ 4. BERSIHkan kolom temp & simpan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = df.drop(columns=[\"sent_len\"])\n",
    "df.to_csv(\"alias_sentence_features_wc.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ  Kolom word_count (total kata per tokoh di cerita) sudah ditambahkan dan disimpan ke 'alias_sentence_features_wc.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed08f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  File disimpan sebagai 'alias_sentence_features_sorted.csv' dengan urutan Tokoh-1, Tokoh-2, dst per cerita.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ‚îÄ‚îÄ 1. LOAD file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = pd.read_csv(\"alias_sentence_features_wc.csv\")\n",
    "\n",
    "# ‚îÄ‚îÄ 2. EXTRACT NOMOR TOKOH (Tokoh-7 ‚Üí 7) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df[\"person_num\"] = (\n",
    "    df[\"person\"]\n",
    "      .str.extract(r\"Tokoh-(\\d+)\", expand=False)\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ 3. SORT: story_id  ‚Üí nomor tokoh  ‚Üí sentence_id ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = (\n",
    "    df.sort_values([\"story_id\", \"person_num\", \"sentence_id\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ 4. DROP kolom bantu & SIMPAN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = df.drop(columns=[\"person_num\"])\n",
    "df.to_csv(\"alias_sentence_features_sorted.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ  File disimpan sebagai 'alias_sentence_features_sorted.csv' dengan urutan Tokoh-1, Tokoh-2, dst per cerita.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97921af8",
   "metadata": {},
   "source": [
    "## add BERT context, ML context, text next, text prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfc4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  text_prev, text_next, bert_context ditambahkan (pakai tokenized_sentences.csv) ‚ûú alias_sentence_features_context.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# ‚îÄ‚îÄ 1.  LOAD dua file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "feat = pd.read_csv(\"alias_sentence_features_wc.csv\")    # story_id | person | ... | text\n",
    "feat[\"aliases\"] = feat[\"aliases\"].apply(ast.literal_eval)\n",
    "\n",
    "tok  = pd.read_csv(\"tokenized_sentences.csv\")           # story_id | judul | sentence_id | word\n",
    "tok   = tok.dropna(subset=[\"word\"]).copy()\n",
    "tok[\"word\"] = tok[\"word\"].astype(str)\n",
    "\n",
    "# ‚îÄ‚îÄ 2.  SUSUN kalimat utuh & lookup‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "sent_df = (\n",
    "    tok.groupby([\"story_id\", \"sentence_id\"])[\"word\"]\n",
    "       .agg(\" \".join)\n",
    "       .reset_index(name=\"full_text\")\n",
    ")\n",
    "\n",
    "lookup = {(r.story_id, r.sentence_id): r.full_text for r in sent_df.itertuples()}\n",
    "\n",
    "def get_prev(row):\n",
    "    return lookup.get((row.story_id, row.sentence_id - 1), \"\")\n",
    "\n",
    "def get_next(row):\n",
    "    return lookup.get((row.story_id, row.sentence_id + 1), \"\")\n",
    "\n",
    "# ‚îÄ‚îÄ 3.  TAMBAH kolom context ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "feat[\"text_prev\"] = feat.apply(get_prev, axis=1)\n",
    "feat[\"text_next\"] = feat.apply(get_next, axis=1)\n",
    "\n",
    "feat[\"bert_context\"] = (\n",
    "    feat[\"text_prev\"].str.strip() + \" [SEP] \" +\n",
    "    feat[\"text\"].str.strip()      + \" [SEP] \" +\n",
    "    feat[\"text_next\"].str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# ‚îÄ‚îÄ 4.  SIMPAN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "feat.to_csv(\"alias_sentence_features_context.csv\", index=False)\n",
    "print(\"‚úÖ  text_prev, text_next, bert_context ditambahkan (pakai tokenized_sentences.csv) ‚ûú alias_sentence_features_context.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c14369",
   "metadata": {},
   "source": [
    "## add is primary in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625787e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  is_primary_in_sentence diperbaiki & file di-sort. Hasil: 'alias_sentence_features_primary_sorted.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# ‚îÄ‚îÄ 1. LOAD the current feature file (with text, aliases, etc.) ‚îÄ‚îÄ\n",
    "df = pd.read_csv(\"alias_sentence_features_context.csv\")   # ganti nama jika berbeda\n",
    "df[\"aliases\"] = df[\"aliases\"].apply(ast.literal_eval)      # list asli\n",
    "\n",
    "# ‚îÄ‚îÄ 2. Kumpulkan SEMUA alias, buat regex word-boundary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "all_aliases = (\n",
    "    df[\"aliases\"]\n",
    "      .explode()\n",
    "      .dropna()\n",
    "      .map(lambda x: x.lower().strip())\n",
    "      .unique()\n",
    "      .tolist()\n",
    ")\n",
    "alias_patterns = {a: re.compile(r\"\\b\" + re.escape(a) + r\"\\b\") for a in all_aliases}\n",
    "\n",
    "# ‚îÄ‚îÄ 3. Tentukan apakah tokoh ini alias PERTAMA di kalimat ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def is_primary(row):\n",
    "    sent = str(row[\"text\"]).lower()\n",
    "    first_pos, first_alias = None, None\n",
    "    for alias, pat in alias_patterns.items():\n",
    "        m = pat.search(sent)\n",
    "        if m:\n",
    "            pos = m.start()\n",
    "            if first_pos is None or pos < first_pos:\n",
    "                first_pos, first_alias = pos, alias\n",
    "    if first_alias is None:\n",
    "        return 0\n",
    "    return 1 if first_alias in [a.lower().strip() for a in row[\"aliases\"]] else 0\n",
    "\n",
    "df[\"is_primary_in_sentence\"] = df.apply(is_primary, axis=1)\n",
    "\n",
    "# ‚îÄ‚îÄ 4. Sort: story_id ‚Üí nomor Tokoh ‚Üí sentence_id ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df[\"person_num\"] = (\n",
    "    df[\"person\"]\n",
    "      .str.extract(r\"Tokoh-(\\d+)\", expand=False)\n",
    "      .astype(int)\n",
    ")\n",
    "df = (\n",
    "    df.sort_values([\"story_id\", \"person_num\", \"sentence_id\"])\n",
    "      .reset_index(drop=True)\n",
    "      .drop(columns=[\"person_num\"])\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ 5. Simpan hasil ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df.to_csv(\"alias_sentence_features_primary_sorted.csv\", index=False)\n",
    "print(\"‚úÖ  is_primary_in_sentence diperbaiki & file di-sort. Hasil: 'alias_sentence_features_primary_sorted.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
