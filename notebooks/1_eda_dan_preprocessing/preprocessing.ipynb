{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rayssa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    story_id               judul  sentence_id      word\n",
      "0          1  Legenda Danau Toba            0        Di\n",
      "1          1  Legenda Danau Toba            0    sebuah\n",
      "2          1  Legenda Danau Toba            0      desa\n",
      "3          1  Legenda Danau Toba            0        di\n",
      "4          1  Legenda Danau Toba            0   wilayah\n",
      "5          1  Legenda Danau Toba            0  Sumatera\n",
      "6          1  Legenda Danau Toba            0         ,\n",
      "7          1  Legenda Danau Toba            0  hiduplah\n",
      "8          1  Legenda Danau Toba            0   seorang\n",
      "9          1  Legenda Danau Toba            0    petani\n",
      "10         1  Legenda Danau Toba            0      yang\n",
      "11         1  Legenda Danau Toba            0     rajin\n",
      "12         1  Legenda Danau Toba            0   bekerja\n",
      "13         1  Legenda Danau Toba            0  meskipun\n",
      "14         1  Legenda Danau Toba            0     lahan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download NLTK tokenizer (only needs to run once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# --- Step 1: Define cleaning functions ---\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert tabs and weird encodings to space\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace('\"\"', '\"')  # Excel-style double quotes\n",
    "    text = text.replace('\",', '')\n",
    "    text = text.replace(',\"', '')\n",
    "    text = text.replace('\",\"', '')\n",
    "    text = text.replace('“', '\"').replace('”', '\"').replace('’', \"'\")\n",
    "\n",
    "    # Remove comma right before or after a quote\n",
    "    text = re.sub(r'\"\\s*,\\s*', ' ', text)\n",
    "    text = re.sub(r'\\s*,\\s*\"', ' ', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove wrapping quotes\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def standarize_quotation(input_df):\n",
    "    quotation_marks = ['“', '”', \"''\", \"``\", \"’’\", \"’\", '\"\"\"']\n",
    "    for mark in quotation_marks:\n",
    "        input_df['text'] = input_df['text'].str.replace(mark, '\"', regex=False)\n",
    "    return input_df\n",
    "\n",
    "# --- Step 2: Load the CSV ---\n",
    "\n",
    "df = pd.read_csv(\"Data Cerita Rakyat.csv\")\n",
    "\n",
    "# Only keep needed columns\n",
    "df = df[['no', 'judul', 'text']].rename(columns={'no': 'story_id'})\n",
    "\n",
    "# Standardize quotes\n",
    "df = standarize_quotation(df)\n",
    "\n",
    "# Clean each text field\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# --- Step 3: Tokenization ---\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    story_id = row['story_id']\n",
    "    title = row['judul']\n",
    "    text = row['text']\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            rows.append({\n",
    "                'story_id': story_id,\n",
    "                'judul': title,\n",
    "                'sentence_id': sent_id,\n",
    "                'word': word\n",
    "            })\n",
    "\n",
    "# --- Step 4: Final clean-up & save ---\n",
    "\n",
    "result_df = pd.DataFrame(rows)\n",
    "\n",
    "# Remove leftover junk quote tokens\n",
    "junk_tokens = ['\"', '`']\n",
    "result_df = result_df[~result_df['word'].isin(junk_tokens)]\n",
    "\n",
    "# Save to file\n",
    "result_df.to_csv(\"cerita_rakyat_tokenized_clean.csv\", index=False)\n",
    "\n",
    "# Optional: Show sample\n",
    "print(result_df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 14067\n"
     ]
    }
   ],
   "source": [
    "# After your tokenization and cleaning\n",
    "\n",
    "# Count how many unique (story_id, sentence_id) pairs\n",
    "n_sentences = result_df[['story_id', 'sentence_id']].drop_duplicates().shape[0]\n",
    "\n",
    "print(f\"Total number of sentences: {n_sentences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of sentences per story: 63.36\n"
     ]
    }
   ],
   "source": [
    "# --- Count sentences per story ---\n",
    "\n",
    "# Step 1: Count unique sentence_id per story_id\n",
    "sentences_per_story = result_df[['story_id', 'sentence_id']].drop_duplicates().groupby('story_id').size()\n",
    "\n",
    "# Step 2: Calculate the mean\n",
    "mean_sentences_per_story = sentences_per_story.mean()\n",
    "\n",
    "print(f\"Mean number of sentences per story: {mean_sentences_per_story:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
